{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b9766db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "from torchvision import datasets, transforms, utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a409aa-4066-46a6-8ba8-43150d068d7c",
   "metadata": {},
   "source": [
    "# Implementing classic VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6712d5aa",
   "metadata": {},
   "source": [
    "We start by creating the VAE class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cb495bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Define the layers\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3_mean = nn.Linear(hidden_dim2, latent_dim)\n",
    "        self.fc3_var = nn.Linear(hidden_dim2, latent_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.elu(self.fc1(x))\n",
    "        out = F.elu(self.fc2(out))\n",
    "        mean = self.fc3_mean(out)\n",
    "        var = self.fc3_var(out) # = diagonal elements of the covariance matrix (mean-field assumption on the latent variables so it can be expressed as a vector)\n",
    "        \n",
    "        return mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b31286",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Define the layers\n",
    "        \n",
    "        self.fc1 = nn.Linear(latent_dim, hidden_dim2)\n",
    "        self.fc2 = nn.Linear(hidden_dim2, hidden_dim1)\n",
    "        self.fc3 = nn.Linear(hidden_dim1, input_dim)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        out = F.elu(self.fc1(z))\n",
    "        out = F.elu(self.fc2(out))\n",
    "        x = torch.sigmoid(self.fc3(out))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7636ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def reparameterization(self, mean, var):\n",
    "        std = torch.exp(var) # To ensure we cannot have negative values in the variance\n",
    "        sample = torch.randn_like(std)\n",
    "        \n",
    "        return mean + std * sample\n",
    "    \n",
    "    def loss(self, recon_x, x, mean, var):\n",
    "        reconstruction_loss = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum') # We use cross entropy as pixels in binary MNIST follows a Bernoulli distribution \n",
    "        kl_divergence = - 0.5 * torch.sum(1 + var - mean.pow(2) - var.exp())\n",
    "\n",
    "        return reconstruction_loss + kl_divergence\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean, var = self.encoder(x)\n",
    "        z = self.reparameterization(mean, var)\n",
    "        recon_x = self.decoder(z)\n",
    "        \n",
    "        return recon_x, mean, var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48205c50",
   "metadata": {},
   "source": [
    "We import MNIST and transform it into binary MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5effc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "# To transform MNIST into binary MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: torch.where(x >= 0.5, torch.tensor(1, dtype=torch.float32), torch.tensor(0, dtype=torch.float32))),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "n_class = len(train_dataset.classes) # Useful for the multi_encoder VAE\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2eef37",
   "metadata": {},
   "source": [
    "We train the VAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89612e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(vae_model, input_dim, loader, optimizer, n_epochs=20):\n",
    "    \"\"\"\n",
    "    Train the VAE\n",
    "\n",
    "    Params:\n",
    "    - vae_model: An instance of VAE\n",
    "    - input_dim: The dimension of the input\n",
    "    - loader: The loader on which we will train the model\n",
    "    - optimizer: The optimizer used to train\n",
    "    - n_epochs: The number of training epochs\n",
    "    \"\"\"\n",
    "    vae_model.train()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        overall_loss = 0\n",
    "\n",
    "        for batch_idx, (x, _) in enumerate(loader):\n",
    "            x = x.view(batch_size, input_dim)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            recon_x, mean, var = vae_model(x)\n",
    "            loss = vae_model.loss(recon_x, x, mean, var)\n",
    "            overall_loss += loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"\\tEpoch\", epoch + 1, \"\\tAverage Loss: \", overall_loss/(batch_idx*batch_size))\n",
    "        \n",
    "    return overall_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0a18d02",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'save/classic_vae.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(classic_vae\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave/classic_vae.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m     classic_vae\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msave/classic_vae.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m classic_vae\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m     23\u001b[0m     param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \n",
      "File \u001b[0;32m~/Documents/MVA/BML/venv/lib/python3.10/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/Documents/MVA/BML/venv/lib/python3.10/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/Documents/MVA/BML/venv/lib/python3.10/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'save/classic_vae.pth'"
     ]
    }
   ],
   "source": [
    "train_classic_vae = False # Change to true if you want to train the VAE instead of taking the trained one\n",
    "\n",
    "# Define the dimension of the layers\n",
    "input_dim = 28 * 28\n",
    "hidden_dim1 = hidden_dim2 = 200\n",
    "latent_dim = 50\n",
    "\n",
    "# Create the classic VAE\n",
    "encoder = Encoder(input_dim, hidden_dim1, hidden_dim2, latent_dim)\n",
    "decoder = Decoder(input_dim, hidden_dim1, hidden_dim2, latent_dim)\n",
    "classic_vae = VAE(encoder, decoder)\n",
    "\n",
    "# Training (if train == True) otherwise just load the already trained model\n",
    "optimizer = torch.optim.Adam(list(classic_vae.encoder.parameters()) + list(classic_vae.decoder.parameters()), lr=0.001)\n",
    "\n",
    "if train_classic_vae:\n",
    "    train(classic_vae, input_dim, train_loader, optimizer)\n",
    "    torch.save(classic_vae.state_dict(), 'save/classic_vae.pth')\n",
    "else:\n",
    "    classic_vae.load_state_dict(torch.load('save/classic_vae.pth'))\n",
    "\n",
    "for param in classic_vae.encoder.parameters():\n",
    "    param.requires_grad = False \n",
    "for param in classic_vae.decoder.parameters():\n",
    "    param.requires_grad = False \n",
    "\n",
    "classic_vae.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4707963",
   "metadata": {},
   "source": [
    "# Implementing WW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5875c466",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_WW(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Define the layers\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3_mean = nn.Linear(hidden_dim2, latent_dim)\n",
    "        self.fc3_var = nn.Linear(hidden_dim2, latent_dim)\n",
    "        \n",
    "    def loss(self, z_samples, mean, var):\n",
    "        normal_dist = dist.MultivariateNormal(mean, torch.diag_embed(torch.exp(var)))\n",
    "        loss = normal_dist.log_prob(z_samples)\n",
    "        n_samples = z_samples.size(0)\n",
    "        \n",
    "        return -torch.sum(loss)/n_samples\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.elu(self.fc1(x))\n",
    "        out = F.elu(self.fc2(out))\n",
    "        mean = self.fc3_mean(out)\n",
    "        var = self.fc3_var(out) # = diagonal elements of the covariance matrix (mean-field assumption on the latent variables so it can be expressed as a vector)\n",
    "        \n",
    "        return mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff109822",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_WW(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Define the layers\n",
    "        \n",
    "        self.fc1 = nn.Linear(latent_dim, hidden_dim2)\n",
    "        self.fc2 = nn.Linear(hidden_dim2, hidden_dim1)\n",
    "        self.fc3 = nn.Linear(hidden_dim1, input_dim)\n",
    "        \n",
    "    def loss(self,recon_x, x):\n",
    "        loss = torch.empty_like(recon_x)\n",
    "\n",
    "        x_expanded = x.repeat(recon_x.size(0),1,1)\n",
    "\n",
    "        loss[x_expanded == 1] = torch.log(recon_x[x_expanded == 1])\n",
    "        loss[x_expanded == 0] = torch.log(1 - recon_x[x_expanded == 0])\n",
    "        \n",
    "        n_samples = loss.size(0)\n",
    "        loss = torch.sum(loss, dim=0)/n_samples\n",
    "\n",
    "        return -torch.sum(loss)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        out = F.elu(self.fc1(z))\n",
    "        out = F.elu(self.fc2(out))\n",
    "        x = torch.sigmoid(self.fc3(out))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d91ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WW(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def reparameterization(self, mean, var):\n",
    "        std = torch.exp(var) # To ensure we cannot have negative values in the variance\n",
    "        sample = torch.randn_like(std)\n",
    "        \n",
    "        return mean + std * sample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean, var = self.encoder(x)\n",
    "        z = self.reparameterization(mean, var)\n",
    "        recon_x = self.decoder(z)\n",
    "        \n",
    "        return recon_x, mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b69d2a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_WW(vae_model, input_dim, loader, optimizer_encoder, optimizer_decoder, n_epochs=20, n_samples=10):\n",
    "    \"\"\"\n",
    "    WW training\n",
    "\n",
    "    Params:\n",
    "    - vae_model: An instance of WW\n",
    "    - input_dim: The dimension of the input\n",
    "    - loader: The loader on which we will train the model\n",
    "    - optimizer_encoder: The optimizer used to train the encoder\n",
    "    - optimizer_decoder: The optimizer used to train the decoder\n",
    "    - n_epochs: The number of training epochs\n",
    "    \"\"\"\n",
    "    vae_model.train()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        overall_loss = 0\n",
    "\n",
    "        for batch_idx, (x, _) in enumerate(loader):\n",
    "            x = x.view(batch_size, input_dim)\n",
    "            \n",
    "            # Decoder Wake phase\n",
    "            optimizer_decoder.zero_grad()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                mean, var = vae_model.encoder(x)\n",
    "\n",
    "                samples = torch.unsqueeze(vae_model.reparameterization(mean, var),0)\n",
    "                for _ in range(n_samples - 1):\n",
    "                    samples = torch.cat((samples,torch.unsqueeze(vae_model.reparameterization(mean, var),0)), dim=0)\n",
    "            \n",
    "            recon_x = torch.unsqueeze(vae_model.decoder(samples[0]),0)\n",
    "\n",
    "            for sample in samples[1:]:\n",
    "                recon_x = torch.cat((recon_x, torch.unsqueeze(vae_model.decoder(samples[0]),0)), dim=0)\n",
    "            \n",
    "            loss = vae_model.decoder.loss(recon_x, x)\n",
    "            overall_loss += loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer_decoder.step()\n",
    "            \n",
    "            # Encoder wake phase\n",
    "            optimizer_encoder.zero_grad()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                z_samples = torch.randn(n_samples,vae_model.decoder.latent_dim)\n",
    "                recon_x = vae_model.decoder(z_samples)\n",
    "                uniform_random = torch.rand_like(recon_x)\n",
    "                x_samples = torch.where(uniform_random > recon_x, torch.tensor(0.), torch.tensor(1.))\n",
    "\n",
    "            mean, var = vae_model.encoder(x_samples)\n",
    "            loss = vae_model.encoder.loss(z_samples, mean, var)\n",
    "            overall_loss += loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer_encoder.step()\n",
    "\n",
    "        print(\"\\tEpoch\", epoch + 1, \"\\tAverage Loss: \", overall_loss/(batch_idx*batch_size))\n",
    "        \n",
    "    return overall_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eff91045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 1 \tAverage Loss:  tensor(282.7353, grad_fn=<DivBackward0>)\n",
      "\tEpoch 2 \tAverage Loss:  tensor(207.9039, grad_fn=<DivBackward0>)\n",
      "\tEpoch 3 \tAverage Loss:  tensor(202.7370, grad_fn=<DivBackward0>)\n",
      "\tEpoch 4 \tAverage Loss:  tensor(194.3177, grad_fn=<DivBackward0>)\n",
      "\tEpoch 5 \tAverage Loss:  tensor(192.0123, grad_fn=<DivBackward0>)\n",
      "\tEpoch 6 \tAverage Loss:  tensor(187.2543, grad_fn=<DivBackward0>)\n",
      "\tEpoch 7 \tAverage Loss:  tensor(178.1110, grad_fn=<DivBackward0>)\n",
      "\tEpoch 8 \tAverage Loss:  tensor(167.5507, grad_fn=<DivBackward0>)\n",
      "\tEpoch 9 \tAverage Loss:  tensor(159.8084, grad_fn=<DivBackward0>)\n",
      "\tEpoch 10 \tAverage Loss:  tensor(152.5081, grad_fn=<DivBackward0>)\n",
      "\tEpoch 11 \tAverage Loss:  tensor(146.0467, grad_fn=<DivBackward0>)\n",
      "\tEpoch 12 \tAverage Loss:  tensor(141.6624, grad_fn=<DivBackward0>)\n",
      "\tEpoch 13 \tAverage Loss:  tensor(138.1333, grad_fn=<DivBackward0>)\n",
      "\tEpoch 14 \tAverage Loss:  tensor(135.7734, grad_fn=<DivBackward0>)\n",
      "\tEpoch 15 \tAverage Loss:  tensor(133.9471, grad_fn=<DivBackward0>)\n",
      "\tEpoch 16 \tAverage Loss:  tensor(132.1696, grad_fn=<DivBackward0>)\n",
      "\tEpoch 17 \tAverage Loss:  tensor(130.4583, grad_fn=<DivBackward0>)\n",
      "\tEpoch 18 \tAverage Loss:  tensor(128.4960, grad_fn=<DivBackward0>)\n",
      "\tEpoch 19 \tAverage Loss:  tensor(126.0010, grad_fn=<DivBackward0>)\n",
      "\tEpoch 20 \tAverage Loss:  tensor(124.1373, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WW(\n",
       "  (encoder): Encoder_WW(\n",
       "    (fc1): Linear(in_features=784, out_features=200, bias=True)\n",
       "    (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (fc3_mean): Linear(in_features=200, out_features=50, bias=True)\n",
       "    (fc3_var): Linear(in_features=200, out_features=50, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder_WW(\n",
       "    (fc1): Linear(in_features=50, out_features=200, bias=True)\n",
       "    (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (fc3): Linear(in_features=200, out_features=784, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ww = True\n",
    "\n",
    "# Define the dimension of the layers\n",
    "input_dim = 28 * 28\n",
    "hidden_dim1 = hidden_dim2 = 200\n",
    "latent_dim = 50\n",
    "\n",
    "# Create the WW\n",
    "encoder_ww = Encoder_WW(input_dim, hidden_dim1, hidden_dim2, latent_dim)\n",
    "decoder_ww = Decoder_WW(input_dim, hidden_dim1, hidden_dim2, latent_dim)\n",
    "ww = WW(encoder_ww, decoder_ww)\n",
    "\n",
    "# Training (if train == True) otherwise just load the already trained model\n",
    "optimizer_encoder = torch.optim.Adam(ww.encoder.parameters(), lr=0.0001)\n",
    "optimizer_decoder = torch.optim.Adam(ww.decoder.parameters(), lr=0.0001)\n",
    "\n",
    "if train_ww:\n",
    "    train_WW(ww, input_dim, train_loader, optimizer_encoder, optimizer_decoder)\n",
    "    torch.save(ww.state_dict(), 'save/ww.pth')\n",
    "else:\n",
    "    ww.load_state_dict(torch.load('save/ww.pth'))\n",
    "\n",
    "for param in ww.encoder.parameters():\n",
    "    param.requires_grad = False \n",
    "for param in ww.decoder.parameters():\n",
    "    param.requires_grad = False \n",
    "\n",
    "ww.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2f85dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original digit\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed digit classic vae\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAKV0lEQVR4nO3cPYvcZRvG4Xt2ZrOZEPFllWxA0BBRJG5jYRAUhBQWfgebgF9EEAvtrWxstRJs1BSSKoKFhSKiBExYJ2/EhM3qZnb+T3dWFnvdkHGf3eOoczKTySy//Re5RsMwDA0AWmsr//UbAODgEAUAQhQACFEAIEQBgBAFAEIUAAhRACAm+/2Do9HoUb4PAB6x/fxfZU8KAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQk//6DRwFo9Foaa81mdT/SY8dO1be7O7uljd7e3vlTWutDcOwlE2Pnn/blZW+38XG43F50/OZ93x2i8WivOFg8qQAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEEf6IF7PgbGTJ0+WN08++WR5s76+Xt601trq6mp50/N3euaZZ8qbra2t8qa11nZ2dsqb+/fvlzfPPfdcefPiiy+WN48//nh501prZ8+eLW9ms1l58+uvv5Y33377bXlz7dq18qa1vmOM7J8nBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQDi0FxJHY1G5c2JEyfKmxdeeKG8efXVV8ubN954o7xpre/9bWxslDfHjx8vb3rduXOnvNne3i5veq7ZPvXUU+VNz3e1tdaOHTtW3uzt7ZU3f/75Z3nT89l9+umn5U1rrd2+fbu8GYah67WOIk8KAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCAHGkD+L1HBh74oknyptnn322vNnc3CxvWus7bjedTsubyaT+1VksFuVNa63t7OyUNz3vr+dz6Pnera6uljet9b2/noN4p0+fLm/OnDlT3qys9P1OOh6Py5v5fN71WkeRJwUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAODQH8YZhKG92d3fLm9lsVt5sb2+XN9evXy9vWus7/NVzYOzu3bvlzaVLl8qb1lq7detWefPXX3+VN//8809503OA8O233y5vWmvtlVdeKW9OnTrV9VpVN2/eLG96vnet9f2ss3+eFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQDiSB/E29nZKW96DuJ999135U3vQbyzZ8+WN/fu3StvLl++XN788ccf5U1rfQcFe44dTqfT8mZ9fb286XlvrbV27ty5rl3V7du3y5sffvhhKa/TWmuLxaJrx/54UgAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgDs2V1B57e3vlzd27d8ub33//fSmv01prv/32W3lz48aN8qbnwmXPtdPW+q5i9lzN7dmcPHmyvHnppZfKm9Za29jYKG8mk/qPeM/39fvvvy9v5vN5ecOj50kBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAII70QbyeA2ij0ai8uXfvXnnTcwSutb7jdru7u0vZ9HzerfV95st6naeffrq8ef3118ub1lqbTqflTc9hxc8++6y8mc1m5U3v96F3x/54UgAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFACII30Qr8fDhw/Lm55Daz2HzFprbTwelzfz+bzrtQ6blZX670hnzpwpb15++eXyprW+I4lXrlwpb77++uvyxpG6w8OTAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAEA4iFe0rMNfe3t7Xbueo2nL+jv1HAZcprW1tfLmwoUL5c1k0vdjN5vNypuPP/64vLlx40Z50/t95eDxpABAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQDuItwbIOzi3ztXqO2y3zc+h5f6dPny5v3nrrrfJmZaXvd7FvvvmmvLly5Up547jd0eZJAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYBwJZUuy7x42mNtba28ee+998qb559/vrzZ2toqb1pr7f333y9v7t+/3/VaHF2eFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQDCQTwOpY2NjfLm3XffLW96DgN+/vnn5U1rrV29erVrBxWeFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQDCQTwOvPF4XN589NFH5c36+np5s7W1Vd588skn5U1rrS0Wi64dVHhSACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAgH8TjwNjc3y5t33nmnvBmGobz55Zdfypvr16+XN7AsnhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAwkE8lmZtba1r98EHH5Q3x48fL2/m83l58+GHH5Y3i8WivIFl8aQAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQLiSSpfRaFTenD9/vuu13nzzzfKm5xLpTz/9VN78+OOP5c0wDOUNLIsnBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYBwEI8u0+m0vLl48eIjeCf/bnt7u7z54osvypubN2+WN3CQeVIAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACAfxaJNJ/WuwublZ3pw/f768aa21hw8fljez2ay8+eqrr8qbYRjKGzjIPCkAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhIN4tNXV1fLmtddeK28ee+yx8qa11lZW6r+7/Pzzz+XNgwcPypvFYlHewEHmSQGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAcCX1kBmNRuXNdDotb8bjcXlz7dq18qbXl19+Wd5cvXq1vBmGobyBg8yTAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECMhn1e9Oo5tMb/h57jdidOnHgE7+Tf/f333+XNfD4vbxy347Dbz3fckwIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBATPb7Bx0LAzj8PCkAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQ/wN1Dq0uIg/KYAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_digit(digit):\n",
    "    # Digit must be a tensor of size 28x28\n",
    "    plt.imshow(digit, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def generate_digit(decoder, sample):\n",
    "    with torch.no_grad():\n",
    "        gen_x = decoder(sample)\n",
    "    digit = gen_x.reshape(28, 28) # reshape vector to 2d array\n",
    "    \n",
    "    return digit\n",
    "\n",
    "x_batch, y_batch = next(iter(test_loader))\n",
    "x = x_batch.view(batch_size, classic_vae.encoder.input_dim)[0]\n",
    "y = y_batch[0]\n",
    "\n",
    "print(\"Original digit\")\n",
    "show_digit(x.reshape(28,28))\n",
    "\n",
    "with torch.no_grad():\n",
    "    mean, var = ww.encoder(x)\n",
    "recon_x = generate_digit(ww.decoder, ww.reparameterization(mean, var))\n",
    "print(\"Reconstructed digit classic vae\")\n",
    "show_digit(recon_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9f40f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
