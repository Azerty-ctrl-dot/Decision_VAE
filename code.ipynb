{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b9766db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "from torchvision import datasets, transforms, utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a409aa-4066-46a6-8ba8-43150d068d7c",
   "metadata": {},
   "source": [
    "# Implementing classic VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6712d5aa",
   "metadata": {},
   "source": [
    "We start by creating the VAE class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cb495bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Define the layers\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3_mean = nn.Linear(hidden_dim2, latent_dim)\n",
    "        self.fc3_var = nn.Linear(hidden_dim2, latent_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.elu(self.fc1(x))\n",
    "        out = F.elu(self.fc2(out))\n",
    "        mean = self.fc3_mean(out)\n",
    "        var = self.fc3_var(out) # = diagonal elements of the covariance matrix (mean-field assumption on the latent variables so it can be expressed as a vector)\n",
    "        \n",
    "        return mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b31286",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Define the layers\n",
    "        \n",
    "        self.fc1 = nn.Linear(latent_dim, hidden_dim2)\n",
    "        self.fc2 = nn.Linear(hidden_dim2, hidden_dim1)\n",
    "        self.fc3 = nn.Linear(hidden_dim1, input_dim)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        out = F.elu(self.fc1(z))\n",
    "        out = F.elu(self.fc2(out))\n",
    "        x = torch.sigmoid(self.fc3(out))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7636ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def reparameterization(self, mean, var):\n",
    "        std = torch.exp(var) # To ensure we cannot have negative values in the variance\n",
    "        sample = torch.randn_like(std)\n",
    "        \n",
    "        return mean + std * sample\n",
    "    \n",
    "    def loss(self, recon_x, x, mean, var):\n",
    "        reconstruction_loss = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum') # We use cross entropy as pixels in binary MNIST follows a Bernoulli distribution \n",
    "        kl_divergence = - 0.5 * torch.sum(1 + var - mean.pow(2) - var.exp())\n",
    "\n",
    "        return reconstruction_loss + kl_divergence\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean, var = self.encoder(x)\n",
    "        z = self.reparameterization(mean, var)\n",
    "        recon_x = self.decoder(z)\n",
    "        \n",
    "        return recon_x, mean, var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48205c50",
   "metadata": {},
   "source": [
    "We import MNIST and transform it into binary MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5effc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "# To transform MNIST into binary MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: torch.where(x >= 0.5, torch.tensor(1, dtype=torch.float32), torch.tensor(0, dtype=torch.float32))),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "n_class = len(train_dataset.classes) # Useful for the multi_encoder VAE\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2eef37",
   "metadata": {},
   "source": [
    "We train the VAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89612e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(vae_model, input_dim, loader, optimizer, n_epochs=20):\n",
    "    \"\"\"\n",
    "    Train the VAE\n",
    "\n",
    "    Params:\n",
    "    - vae_model: An instance of VAE\n",
    "    - input_dim: The dimension of the input\n",
    "    - loader: The loader on which we will train the model\n",
    "    - optimizer: The optimizer used to train\n",
    "    - n_epochs: The number of training epochs\n",
    "    \"\"\"\n",
    "    vae_model.train()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        overall_loss = 0\n",
    "\n",
    "        for batch_idx, (x, _) in enumerate(loader):\n",
    "            x = x.view(batch_size, input_dim)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            recon_x, mean, var = vae_model(x)\n",
    "            loss = vae_model.loss(recon_x, x, mean, var)\n",
    "            overall_loss += loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"\\tEpoch\", epoch + 1, \"\\tAverage Loss: \", overall_loss/(batch_idx*batch_size))\n",
    "        \n",
    "    return overall_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0a18d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 1 \tAverage Loss:  tensor(180.0544, grad_fn=<DivBackward0>)\n",
      "\tEpoch 2 \tAverage Loss:  tensor(118.5164, grad_fn=<DivBackward0>)\n",
      "\tEpoch 3 \tAverage Loss:  tensor(99.9547, grad_fn=<DivBackward0>)\n",
      "\tEpoch 4 \tAverage Loss:  tensor(90.4828, grad_fn=<DivBackward0>)\n",
      "\tEpoch 5 \tAverage Loss:  tensor(84.8831, grad_fn=<DivBackward0>)\n",
      "\tEpoch 6 \tAverage Loss:  tensor(81.5570, grad_fn=<DivBackward0>)\n",
      "\tEpoch 7 \tAverage Loss:  tensor(79.3568, grad_fn=<DivBackward0>)\n",
      "\tEpoch 8 \tAverage Loss:  tensor(77.6330, grad_fn=<DivBackward0>)\n",
      "\tEpoch 9 \tAverage Loss:  tensor(76.2236, grad_fn=<DivBackward0>)\n",
      "\tEpoch 10 \tAverage Loss:  tensor(75.2047, grad_fn=<DivBackward0>)\n",
      "\tEpoch 11 \tAverage Loss:  tensor(74.1602, grad_fn=<DivBackward0>)\n",
      "\tEpoch 12 \tAverage Loss:  tensor(73.4176, grad_fn=<DivBackward0>)\n",
      "\tEpoch 13 \tAverage Loss:  tensor(72.7624, grad_fn=<DivBackward0>)\n",
      "\tEpoch 14 \tAverage Loss:  tensor(72.0261, grad_fn=<DivBackward0>)\n",
      "\tEpoch 15 \tAverage Loss:  tensor(71.2458, grad_fn=<DivBackward0>)\n",
      "\tEpoch 16 \tAverage Loss:  tensor(70.6310, grad_fn=<DivBackward0>)\n",
      "\tEpoch 17 \tAverage Loss:  tensor(70.0184, grad_fn=<DivBackward0>)\n",
      "\tEpoch 18 \tAverage Loss:  tensor(69.4780, grad_fn=<DivBackward0>)\n",
      "\tEpoch 19 \tAverage Loss:  tensor(69.0411, grad_fn=<DivBackward0>)\n",
      "\tEpoch 20 \tAverage Loss:  tensor(68.6278, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Encoder(\n",
       "    (fc1): Linear(in_features=784, out_features=200, bias=True)\n",
       "    (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (fc3_mean): Linear(in_features=200, out_features=50, bias=True)\n",
       "    (fc3_var): Linear(in_features=200, out_features=50, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (fc1): Linear(in_features=50, out_features=200, bias=True)\n",
       "    (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (fc3): Linear(in_features=200, out_features=784, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_classic_vae = False # Change to true if you want to train the VAE instead of taking the trained one\n",
    "\n",
    "# Define the dimension of the layers\n",
    "input_dim = 28 * 28\n",
    "hidden_dim1 = hidden_dim2 = 200\n",
    "latent_dim = 50\n",
    "\n",
    "# Create the classic VAE\n",
    "encoder = Encoder(input_dim, hidden_dim1, hidden_dim2, latent_dim)\n",
    "decoder = Decoder(input_dim, hidden_dim1, hidden_dim2, latent_dim)\n",
    "classic_vae = VAE(encoder, decoder)\n",
    "\n",
    "# Training (if train == True) otherwise just load the already trained model\n",
    "optimizer = torch.optim.Adam(list(classic_vae.encoder.parameters()) + list(classic_vae.decoder.parameters()), lr=0.001)\n",
    "\n",
    "if train_classic_vae:\n",
    "    train(classic_vae, input_dim, train_loader, optimizer)\n",
    "    torch.save(classic_vae.state_dict(), 'save/classic_vae.pth')\n",
    "else:\n",
    "    classic_vae.load_state_dict(torch.load('save/classic_vae.pth'))\n",
    "\n",
    "for param in classic_vae.encoder.parameters():\n",
    "    param.requires_grad = False \n",
    "for param in classic_vae.decoder.parameters():\n",
    "    param.requires_grad = False \n",
    "\n",
    "classic_vae.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4707963",
   "metadata": {},
   "source": [
    "# Implementing WW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5875c466",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_WW(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Define the layers\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3_mean = nn.Linear(hidden_dim2, latent_dim)\n",
    "        self.fc3_var = nn.Linear(hidden_dim2, latent_dim)\n",
    "        \n",
    "    def loss(self, z_samples, mean, var):\n",
    "        normal_dist = dist.MultivariateNormal(mean, torch.diag_embed(torch.exp(var)))\n",
    "        loss = normal_dist.log_prob(z_samples)\n",
    "        n_samples = z_samples.size(0)\n",
    "        \n",
    "        return -torch.sum(loss)/n_samples\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.elu(self.fc1(x))\n",
    "        out = F.elu(self.fc2(out))\n",
    "        mean = self.fc3_mean(out)\n",
    "        var = self.fc3_var(out) # = diagonal elements of the covariance matrix (mean-field assumption on the latent variables so it can be expressed as a vector)\n",
    "        \n",
    "        return mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff109822",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_WW(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Define the layers\n",
    "        \n",
    "        self.fc1 = nn.Linear(latent_dim, hidden_dim2)\n",
    "        self.fc2 = nn.Linear(hidden_dim2, hidden_dim1)\n",
    "        self.fc3 = nn.Linear(hidden_dim1, input_dim)\n",
    "        \n",
    "    def loss(self,recon_x, x):\n",
    "        loss = torch.empty_like(recon_x)\n",
    "\n",
    "        x_expanded = x.repeat(recon_x.size(0),1,1)\n",
    "\n",
    "        loss[x_expanded == 1] = torch.log(recon_x[x_expanded == 1])\n",
    "        loss[x_expanded == 0] = torch.log(1 - recon_x[x_expanded == 0])\n",
    "        \n",
    "        n_samples = loss.size(0)\n",
    "        loss = torch.sum(loss, dim=0)/n_samples\n",
    "\n",
    "        return -torch.sum(loss)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        out = F.elu(self.fc1(z))\n",
    "        out = F.elu(self.fc2(out))\n",
    "        x = torch.sigmoid(self.fc3(out))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d91ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WW(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def reparameterization(self, mean, var):\n",
    "        std = torch.exp(var) # To ensure we cannot have negative values in the variance\n",
    "        sample = torch.randn_like(std)\n",
    "        \n",
    "        return mean + std * sample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean, var = self.encoder(x)\n",
    "        z = self.reparameterization(mean, var)\n",
    "        recon_x = self.decoder(z)\n",
    "        \n",
    "        return recon_x, mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b69d2a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_WW(vae_model, input_dim, loader, optimizer_encoder, optimizer_decoder, n_epochs=20, n_samples=10):\n",
    "    \"\"\"\n",
    "    WW training\n",
    "\n",
    "    Params:\n",
    "    - vae_model: An instance of WW\n",
    "    - input_dim: The dimension of the input\n",
    "    - loader: The loader on which we will train the model\n",
    "    - optimizer_encoder: The optimizer used to train the encoder\n",
    "    - optimizer_decoder: The optimizer used to train the decoder\n",
    "    - n_epochs: The number of training epochs\n",
    "    \"\"\"\n",
    "    vae_model.train()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        overall_loss = 0\n",
    "\n",
    "        for batch_idx, (x, _) in enumerate(loader):\n",
    "            x = x.view(batch_size, input_dim)\n",
    "            \n",
    "            # Decoder Wake phase\n",
    "            optimizer_decoder.zero_grad()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                mean, var = vae_model.encoder(x)\n",
    "\n",
    "                samples = torch.unsqueeze(vae_model.reparameterization(mean, var),0)\n",
    "                for _ in range(n_samples - 1):\n",
    "                    samples = torch.cat((samples,torch.unsqueeze(vae_model.reparameterization(mean, var),0)), dim=0)\n",
    "            \n",
    "            recon_x = torch.unsqueeze(vae_model.decoder(samples[0]),0)\n",
    "\n",
    "            for sample in samples[1:]:\n",
    "                recon_x = torch.cat((recon_x, torch.unsqueeze(vae_model.decoder(samples[0]),0)), dim=0)\n",
    "            \n",
    "            loss = vae_model.decoder.loss(recon_x, x)\n",
    "            overall_loss += loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer_decoder.step()\n",
    "            \n",
    "            # Encoder wake phase\n",
    "            optimizer_encoder.zero_grad()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                z_samples = torch.randn(n_samples,vae_model.decoder.latent_dim)\n",
    "                recon_x = vae_model.decoder(z_samples)\n",
    "                uniform_random = torch.rand_like(recon_x)\n",
    "                x_samples = torch.where(uniform_random > recon_x, torch.tensor(0.), torch.tensor(1.))\n",
    "\n",
    "            mean, var = vae_model.encoder(x_samples)\n",
    "            loss = vae_model.encoder.loss(z_samples, mean, var)\n",
    "            overall_loss += loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer_encoder.step()\n",
    "\n",
    "        print(\"\\tEpoch\", epoch + 1, \"\\tAverage Loss: \", overall_loss/(batch_idx*batch_size))\n",
    "        \n",
    "    return overall_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eff91045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 1 \tAverage Loss:  tensor(285.5696, grad_fn=<DivBackward0>)\n",
      "\tEpoch 2 \tAverage Loss:  tensor(208.0087, grad_fn=<DivBackward0>)\n",
      "\tEpoch 3 \tAverage Loss:  tensor(203.6354, grad_fn=<DivBackward0>)\n",
      "\tEpoch 4 \tAverage Loss:  tensor(194.7072, grad_fn=<DivBackward0>)\n",
      "\tEpoch 5 \tAverage Loss:  tensor(191.3087, grad_fn=<DivBackward0>)\n",
      "\tEpoch 6 \tAverage Loss:  tensor(184.1606, grad_fn=<DivBackward0>)\n",
      "\tEpoch 7 \tAverage Loss:  tensor(177.7304, grad_fn=<DivBackward0>)\n",
      "\tEpoch 8 \tAverage Loss:  tensor(174.3477, grad_fn=<DivBackward0>)\n",
      "\tEpoch 9 \tAverage Loss:  tensor(168.1088, grad_fn=<DivBackward0>)\n",
      "\tEpoch 10 \tAverage Loss:  tensor(157.0839, grad_fn=<DivBackward0>)\n",
      "\tEpoch 11 \tAverage Loss:  tensor(148.4409, grad_fn=<DivBackward0>)\n",
      "\tEpoch 12 \tAverage Loss:  tensor(143.0748, grad_fn=<DivBackward0>)\n",
      "\tEpoch 13 \tAverage Loss:  tensor(139.9087, grad_fn=<DivBackward0>)\n",
      "\tEpoch 14 \tAverage Loss:  tensor(137.9241, grad_fn=<DivBackward0>)\n",
      "\tEpoch 15 \tAverage Loss:  tensor(136.2308, grad_fn=<DivBackward0>)\n",
      "\tEpoch 16 \tAverage Loss:  tensor(134.3555, grad_fn=<DivBackward0>)\n",
      "\tEpoch 17 \tAverage Loss:  tensor(132.4981, grad_fn=<DivBackward0>)\n",
      "\tEpoch 18 \tAverage Loss:  tensor(130.6710, grad_fn=<DivBackward0>)\n",
      "\tEpoch 19 \tAverage Loss:  tensor(129.1364, grad_fn=<DivBackward0>)\n",
      "\tEpoch 20 \tAverage Loss:  tensor(127.4626, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WW(\n",
       "  (encoder): Encoder_WW(\n",
       "    (fc1): Linear(in_features=784, out_features=200, bias=True)\n",
       "    (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (fc3_mean): Linear(in_features=200, out_features=50, bias=True)\n",
       "    (fc3_var): Linear(in_features=200, out_features=50, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder_WW(\n",
       "    (fc1): Linear(in_features=50, out_features=200, bias=True)\n",
       "    (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (fc3): Linear(in_features=200, out_features=784, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ww = False # Change to true if you want to train using WW instead of taking the trained one\n",
    "\n",
    "# Define the dimension of the layers\n",
    "input_dim = 28 * 28\n",
    "hidden_dim1 = hidden_dim2 = 200\n",
    "latent_dim = 50\n",
    "\n",
    "# Create the WW\n",
    "encoder_ww = Encoder_WW(input_dim, hidden_dim1, hidden_dim2, latent_dim)\n",
    "decoder_ww = Decoder_WW(input_dim, hidden_dim1, hidden_dim2, latent_dim)\n",
    "ww = WW(encoder_ww, decoder_ww)\n",
    "\n",
    "# Training (if train == True) otherwise just load the already trained model\n",
    "optimizer_encoder = torch.optim.Adam(ww.encoder.parameters(), lr=0.0001)\n",
    "optimizer_decoder = torch.optim.Adam(ww.decoder.parameters(), lr=0.0001)\n",
    "\n",
    "if train_ww:\n",
    "    train_WW(ww, input_dim, train_loader, optimizer_encoder, optimizer_decoder)\n",
    "    torch.save(ww.state_dict(), 'save/ww.pth')\n",
    "else:\n",
    "    ww.load_state_dict(torch.load('save/ww.pth'))\n",
    "\n",
    "for param in ww.encoder.parameters():\n",
    "    param.requires_grad = False \n",
    "for param in ww.decoder.parameters():\n",
    "    param.requires_grad = False \n",
    "\n",
    "ww.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a55facf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original digit\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed digit classic vae\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed digit WW\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_digit(digit):\n",
    "    # Digit must be a tensor of size 28x28\n",
    "    plt.imshow(digit, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def generate_digit(decoder, sample):\n",
    "    with torch.no_grad():\n",
    "        gen_x = decoder(sample)\n",
    "    digit = gen_x.reshape(28, 28) # reshape vector to 2d array\n",
    "    \n",
    "    return digit\n",
    "\n",
    "x_batch, y_batch = next(iter(test_loader))\n",
    "x = x_batch.view(batch_size, classic_vae.encoder.input_dim)[0]\n",
    "y = y_batch[0]\n",
    "\n",
    "print(\"Original digit\")\n",
    "show_digit(x.reshape(28,28))\n",
    "\n",
    "with torch.no_grad():\n",
    "    mean, var = classic_vae.encoder(x)\n",
    "recon_x = generate_digit(classic_vae.decoder, classic_vae.reparameterization(mean, var))\n",
    "print(\"Reconstructed digit classic vae\")\n",
    "show_digit(recon_x)\n",
    "\n",
    "with torch.no_grad():\n",
    "    mean, var = ww.encoder(x)\n",
    "recon_x = generate_digit(ww.decoder, ww.reparameterization(mean, var))\n",
    "print(\"Reconstructed digit WW\")\n",
    "show_digit(recon_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
